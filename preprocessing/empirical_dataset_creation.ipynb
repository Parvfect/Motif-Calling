{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import sort_transcript\n",
    "from evaluation import evaluate_cycle_prediction\n",
    "from transcript_sorting import sort_transcript, create_reduced_spacer_transcript, sort_transcript_reduced_spacers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\datasets\\empirical\\empirical_train_dataset_v6.pkl\"\n",
    "test_dataset_path = r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\datasets\\empirical\\full_empirical_test_dataset_v5_payload_seq.pkl\"\n",
    "motif_search_barcoded = r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\motif_search_barcoded.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(train_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orientation_df = pd.read_csv(motif_search_barcoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_add = [col for col in orientation_df.columns if col not in train_df.columns or col == 'read_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(train_df, orientation_df[cols_to_add], on='read_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_for_barcode_address(df: pd.DataFrame, barcode: int, address: str) -> pd.DataFrame:\n",
    "    return df.loc[\n",
    "        (df['ONT_Barcode'] == barcode) & \n",
    "        (df['HW_Address'] == address) &\n",
    "        (df['orientation'].str.startswith('+'))\n",
    "\n",
    "    ]\n",
    "\n",
    "def sort_sequences_by_length(spacer_sequences: List[str], read_ids: List[str]):\n",
    "\n",
    "    sorted_indices = sorted(\n",
    "        range(len(spacer_sequences)), key=lambda i: len(spacer_sequences[i]), reverse=True)\n",
    "    \n",
    "    return [spacer_sequences[ind] for ind in sorted_indices], [read_ids[ind] for ind in sorted_indices]\n",
    "        \n",
    "\n",
    "def get_longest_sequence(spacer_sequences: List[str], read_ids: List[str]):\n",
    "    max_len = 0\n",
    "    max_seq = 0\n",
    "    read_id = \"\"\n",
    "    for seq, r_id in zip(spacer_sequences, read_ids):\n",
    "        if len(seq) > max_len:\n",
    "            max_len = len(seq)\n",
    "            max_seq = seq\n",
    "            read_id = r_id\n",
    "\n",
    "    return max_seq, read_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seqs_arr = []\n",
    "read_ids_arr = []\n",
    "\n",
    "for barcode in tqdm(merged_df['ONT_Barcode'].unique()):\n",
    "    for address in merged_df['HW_Address'].unique():\n",
    "        selected_df = get_df_for_barcode_address(\n",
    "            df=merged_df, barcode=barcode, address=address)\n",
    "        \n",
    "        #seq, read_id = get_longest_sequence(\n",
    "        #    selected_df['Spacer_Sequence'], selected_df['read_id'])\n",
    "        \n",
    "        seqs, read_ids = sort_sequences_by_length(\n",
    "            selected_df['Spacer_Sequence'].tolist(), selected_df['read_id'].tolist())\n",
    "\n",
    "        \n",
    "        seqs_arr.extend(seqs[:10])\n",
    "        read_ids_arr.extend(read_ids[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reduced_spacer_transcript(motif_seq: List[int]) -> List[int]:\n",
    "    \"\"\" 12 4 12 12 3 12 -> 12 4 2 3 4 12 13 2 4 5 3 13\"\"\"\n",
    "\n",
    "    seq = []\n",
    "    str_seq = \" \".join([str(i) for i in motif_seq])\n",
    "    cycle_transcript = sort_transcript(str_seq)\n",
    "\n",
    "    for ind, i in enumerate(cycle_transcript):\n",
    "        if len(i) == 0:\n",
    "            continue\n",
    "        \n",
    "        seq.append(ind + 11)\n",
    "        seq.extend(list(set(i)))\n",
    "        seq.append(ind + 11)\n",
    "\n",
    "    return seq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in seqs_arr:\n",
    "    sorted_transcript = sort_transcript(i)\n",
    "    reduced_spacers_str = create_reduced_spacer_transcript(i)\n",
    "    reduced_spacers_transcript = sort_transcript_reduced_spacers(reduced_spacers_str)\n",
    "\n",
    "    print(sorted_transcript)\n",
    "    print(reduced_spacers_transcript)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([len(i)/3 for i in seqs_arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = merged_df[merged_df['read_id'].isin(read_ids_arr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.rename(columns={'Spacer_Sequence': 'motif_seq'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['motif_seq'] = filtered_df['motif_seq'].apply(create_reduced_spacer_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_pickle(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\pickled_datasets\\cleaned_+_reduced_spacers_5_per_read.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = pd.read_pickle(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\pickled_datasets\\cleaned_+_reduced_spacers_5_per_cycle.pkl\")\n",
    "\n",
    "\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in filtered_df['motif_seq']:\n",
    "    print(i)\n",
    "    t = sort_transcript_reduced_spacers(\" \".join([str(k) for k in i]))\n",
    "    print(t)\n",
    "    t = [list(set(f)) for f in t]\n",
    "    print(t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filt = pd.read_pickle(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\full_datasets\\master_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filt_ = train_filt.loc[(train_filt['orientation'].str.startswith('-')) & (train_filt['payload_motifs_found'] > 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_filt_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filt_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filt_.to_pickle(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\full_datasets\\reverse_oriented.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filt['payload_motifs_found'] > 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filt_.read_pickle(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\full_datasets\\reverse_oriented.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing reverse oriented labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\full_datasets\\edit_master_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df = df.loc[df['strand'].str.startswith('-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df = rev_df.loc[rev_df['edit_motifs_found'] > 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df['edit_spacer_seq'] = rev_df['edit_spacer_seq'].apply(lambda x: x[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df.to_pickle(r'C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\full_datasets\\edit_train_rev.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing labels for classifer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\edit_distance_motif_search\\edit_train_filtered_reverse.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['edit_spacer_seq'] = df['edit_spacer_seq'].apply(lambda x: x[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing motif search labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_db = pd.read_csv(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\01-04run\\master_db.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = master_db.loc[~master_db['HW_Address'].str.startswith('unclassified')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_functions import get_cleaned_encoded_file\n",
    "from utils import evaluate_prediction, create_spacer_sequence_with_address\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from transcript_sorting import sort_transcript_with_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df = pd.read_csv(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\01-04run\\HELIX01-04-encoded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_cleaned = get_cleaned_encoded_file(encoded_df, address=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(filtered_df, encoded_cleaned, on='HW_Address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_library_motif_transcript(library_prediction, encoded, library_typos=True):\n",
    "    \"\"\"Sorts library motif transcripts while fixing for typos, by looking at the payloads to get the best orientation matches\"\"\"\n",
    "\n",
    "    sorted_prediction = [[] for i in range(10)]\n",
    "\n",
    "    split_library_prediction = library_prediction.split('|')\n",
    "\n",
    "    #print(split_library_prediction)\n",
    "    for i in split_library_prediction:\n",
    "\n",
    "        if len(i) < 8:\n",
    "            continue\n",
    "        # searching for the 10\n",
    "        motif_found = int(i[-1])\n",
    "        cycle_address = None  # starts from 1 there and from 0 here\n",
    "        if i[-4] == '1':\n",
    "            if i[-3] == '0':\n",
    "                cycle_address = 10\n",
    "                sorted_prediction[cycle_address - 1].append(motif_found)\n",
    "        elif i[-4] == '9':\n",
    "            continue\n",
    "        else:\n",
    "            cycle_address = int(i[-3])\n",
    "            sorted_prediction[cycle_address - 1].append(motif_found)\n",
    "\n",
    "    mf, me = evaluate_prediction(sorted_prediction[2:], encoded)\n",
    "    return sorted_prediction, mf, me\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motifs_found = []\n",
    "motif_errs = []\n",
    "sorted_predictions = []\n",
    "spacer_seqs = []\n",
    "read_ids = []\n",
    "for ind, row in tqdm(merged_df.iterrows(), total=len(merged_df)):\n",
    "\n",
    "    library_prediction = row['library_motif']\n",
    "    payload = row['payload']\n",
    "    read_id = row['read_id']\n",
    "    \n",
    "    try:\n",
    "        sorted_pred, mf, me = sort_library_motif_transcript(library_prediction, payload)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    \n",
    "    #if mf > 6:        \n",
    "    motifs_found.append(mf)\n",
    "    motif_errs.append(me)\n",
    "    sorted_predictions.append(sorted_pred)\n",
    "    spacer_seqs.append(create_spacer_sequence_with_address(sorted_pred))\n",
    "    read_ids.append(read_id)\n",
    "\n",
    "    #if ind == 20000:\n",
    "    #    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = merged_df.loc[merged_df['read_id'].isin(read_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['sorted_predictions'] = sorted_predictions\n",
    "filtered_df['motif_seq'] = spacer_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_seqs = pd.read_pickle(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\01-04run\\edit_medium.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_seqs.rename(columns={\"motif_seq\": \"edit_pred\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(filtered_df, edit_seqs, on='read_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing edit search performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_found_arr = []\n",
    "edit_errs_arr = []\n",
    "\n",
    "for ind, row in merged_df.iterrows():\n",
    "\n",
    "    #search_pred = row['sorted_predictions']\n",
    "    edit_pred = row['edit_pred']\n",
    "    payload = row['payload']\n",
    "\n",
    "    edit_found, edit_errs = evaluate_prediction(edit_pred[2:], payload)\n",
    "    \n",
    "    edit_found_arr.append(edit_found)\n",
    "    edit_errs_arr.append(edit_errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['edit_found'] = edit_found_arr\n",
    "merged_df['edit_err'] = edit_errs_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward = merged_df.loc[merged_df['orientation'].str.startswith('+')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_ = merged_df[merged_df['edit_found'] > 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating test and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_barcodes = ['barcode_external04_internal01', 'barcode_external01_internal03', 'barcode_external06_internal01', 'barcode_external01_internal02', 'barcode_external08_internal01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_ = filtered_df_.loc[~filtered_df_['HW_Address'].isin(test_barcodes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squiggle_df = pd.read_pickle(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\01-04run\\master.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(filtered_df_, squiggle_df[['squiggle', 'read_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['edit_spacer_seq'] = merged_df['edit_pred'].apply(lambda x: create_spacer_sequence_with_address(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_pickle(r'C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\01-04run\\finetuning_datasets\\edit_train.pkl')\n",
    "# Switch orientation by filtering out please"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_pickle(r'C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\01-04run\\finetuning_datasets\\1_04_mixed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = master_db.loc[master_db['HW_Address'].isin(test_barcodes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(test_df, encoded_cleaned, on='HW_Address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\01-04run\\finetuning_datasets\\edit_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\01-04run\\finetuning_datasets\\edit_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['orientation'].str.startswith('-'), 'edit_spacer_seq'] = df.loc[df['orientation'].str.startswith('-'), 'edit_spacer_seq'].str[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\01-04run\\finetuning_datasets\\edit_train.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set 01-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_df = pd.read_pickle(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\01-04run\\misc_datasets\\zero_error_search.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_barcodes = search_df['HW_Address'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "master_read_ids = []\n",
    "\n",
    "for barcode in unique_barcodes:\n",
    "    filtered_df = search_df.loc[search_df['HW_Address'].str.startswith(barcode)]\n",
    "\n",
    "    # From this I want to sample - 5000 forward and 5000 reverse\n",
    "    forward_df = filtered_df.loc[filtered_df['strand'].str.startswith('+')]\n",
    "    reads_sampled = forward_df.sample(n=5000)['read_id'].tolist()\n",
    "    master_read_ids.extend(reads_sampled)\n",
    "    \n",
    "    # From this I want to sample - 5000 forward and 5000 reverse\n",
    "    reverse_df = filtered_df.loc[filtered_df['strand'].str.startswith('-')]\n",
    "    reads_sampled = reverse_df.sample(n=5000)['read_id'].tolist()\n",
    "    master_read_ids.extend(reads_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(master_read_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(master_read_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = search_df.loc[search_df['read_id'].isin(master_read_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df['orientation'] = sorted_df['strand'].apply(lambda x: 1 if x.startswith('+') else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making master fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through all the fastq files, extract all the reads that are within the master db and then finally write to another fastq file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from Bio import SeqIO\n",
    "from typing import List\n",
    "\n",
    "def parse_biopython(input_fastq):\n",
    "    for record in SeqIO.parse(input_fastq, 'fastq'):\n",
    "        yield record\n",
    "\n",
    "def get_fastq_records(fastq_filepath):\n",
    "    records = []\n",
    "    for i, record in enumerate((parse_biopython(fastq_filepath))):\n",
    "        records.append(record)\n",
    "    return records\n",
    "\n",
    "def create_fasta_file(ids: List[str], strands: List[str], output_filepath: str):\n",
    "    with open(output_filepath, 'w') as f:\n",
    "        for i, strand in enumerate(strands):\n",
    "            f.write(f\">{ids[i]}\\n\")\n",
    "            f.write(strand + '\\n\\n')\n",
    "\n",
    "    print(f\"File saved as {output_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_basepath = r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\fastq\\01-04\\FASTQ\\pass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume master_read_ids is a list or something iterable\n",
    "master_read_ids_set = set(master_read_ids)  # O(1) lookup\n",
    "\n",
    "def filter_records(records):\n",
    "    return [record for record in records if str(record.id) in master_read_ids_set]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_records = []\n",
    "for file in tqdm(os.listdir(fastq_basepath)):\n",
    "    records = get_fastq_records(os.path.join(fastq_basepath, file))\n",
    "    master_records.extend(filter_records(records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output.fastq\", \"w\") as handle:\n",
    "    SeqIO.write(master_records, handle, \"fastq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random runs dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Steps\n",
    "1. Load master db files into a dataframe\n",
    "2. Merge encoded into the master db (will have to fit to a single run)\n",
    "3. Repeat / Generalise\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\sequencing_runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_filepath = os.path.join(basepath, \"HELIX-01-07-DNA-DECAY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_db_filepath = os.path.join(run_filepath, 'master_db.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_master_db_df(master_db_filepath):\n",
    "    with open(master_db_filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "\n",
    "    data = {\n",
    "        \"read_id\": set(),\n",
    "        \"filename\": [],\n",
    "        \"barcode_1\": [],\n",
    "        \"barcode_2\": [],\n",
    "        \"orientation\": []    \n",
    "    }\n",
    "\n",
    "    for line in lines:\n",
    "        split_line = line.split()\n",
    "\n",
    "        read_id = split_line[0]\n",
    "        filename = split_line[1]\n",
    "        barcode_1 = split_line[2]\n",
    "        barcode_2 = split_line[3]\n",
    "        orientation = split_line[4]\n",
    "\n",
    "        if read_id not in data[\"read_id\"]:\n",
    "            data[\"read_id\"].add(read_id)\n",
    "            data[\"filename\"].append(filename)\n",
    "            data[\"barcode_1\"].append(barcode_1)\n",
    "            data[\"barcode_2\"].append(barcode_2)\n",
    "            data[\"orientation\"].append(orientation)\n",
    "\n",
    "    data['read_id'] = list(data['read_id'])\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_master_db_df(master_db_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df = pd.read_csv(os.path.join(run_filepath, \"encoded.tsv\"), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.loc[~df['barcode_2'].str.startswith('unclassified')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duble_filter = filtered_df.loc[filtered_df['filename'].str.startswith('FAV33791_0ddbb')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duble_filter['filename'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\n",
    "    'FAV33791_0ddbb029_cd7ebfbf_12.fast5',\n",
    "    'FAV33791_0ddbb029_cd7ebfbf_57.fast5',\n",
    "    'FAV33791_0ddbb029_cd7ebfbf_75.fast5',\n",
    "    'FAV33791_0ddbb029_cd7ebfbf_77.fast5',\n",
    "    'FAV33791_0ddbb029_cd7ebfbf_78.fast5'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duble_filter = duble_filter.loc[duble_filter['filename'].isin(filenames)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = encoded_df[['HW_Address', 'payload', 'ONT_Barcode']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_functions import get_cleaned_encoded_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = get_cleaned_encoded_file(encoded_df, address=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duble_filter.rename(columns={\"barcode_1\": \"ONT_Barcode\", \"barcode_2\": \"HW_Address\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duble_filter = duble_filter.loc[~duble_filter['ONT_Barcode'].str.startswith('unclassified')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duble_filter['ONT_Barcode'] = duble_filter['ONT_Barcode'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duble_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc['ONT_Barcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(duble_filter, enc, on=['HW_Address', 'ONT_Barcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now load fast5s\n",
    "\n",
    "from ont_fast5_api.fast5_interface import get_fast5_file\n",
    "\n",
    "def get_data_from_fast5(fast5_filepath, selected_read_ids=None):\n",
    "    raw_data_arr = []\n",
    "    read_ids = []\n",
    "    with get_fast5_file(fast5_filepath, mode=\"r\") as f5:\n",
    "        for read in f5.get_reads():\n",
    "            raw_data = read.get_raw_data()\n",
    "            raw_data_arr.append(raw_data)\n",
    "            read_ids.append(read.read_id)\n",
    "    return raw_data_arr, read_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_read_ids = merged_df['read_id'].to_list()\n",
    "\n",
    "squiggle_df = {i: None for i in read_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selected_read_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in filenames:\n",
    "    raw_data_arr, read_ids_loc = get_data_from_fast5(os.path.join(run_filepath, file))\n",
    "    print(len(set(read_ids_loc).intersection(set(selected_read_ids))))\n",
    "    print(read_ids_loc)\n",
    "\n",
    "    for ind, id in enumerate(read_ids_loc):\n",
    "        squiggle_df[id] = raw_data_arr[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squiggle_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01-13-EXP2-REP3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\sequencing_runs\\01-13\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_search_df = pd.read_csv(os.path.join(basepath, \"full_motif_search.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_df = pd.read_csv(os.path.join(basepath, \"HW_demultiplexing_summary.csv\"))\n",
    "ont_df = pd.read_csv(os.path.join(basepath, \"ONT_demultiplexing_summary.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_df = hw_df[['barcode_arrangement', 'read_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_df.rename(columns={'barcode_arrangement': \"HW_Address\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ont_df = ont_df[['barcode_arrangement', 'read_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ont_df.rename(columns={'barcode_arrangement': 'ONT_Barcode'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging demultiplexed into motif search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(ont_df, hw_df, on='read_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(merged_df, motif_search_df, on='read_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting rid of unclassified reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.loc[~merged_df['ONT_Barcode'].str.startswith('unclassified')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.loc[~merged_df['HW_Address'].str.startswith('unclassified')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['ONT_Barcode'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_csv = pd.read_csv(os.path.join(basepath, 'HELIX01-13-W1_encoded.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_functions import get_cleaned_encoded_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_csv_ = get_cleaned_encoded_file(encoded_csv, address=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['ONT_Barcode'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_csv_['ONT_Barcode'] = encoded_csv_['ONT_Barcode'].apply(lambda x: f\"barcode0{x}\" if x < 9 else f\"barcode{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.merge(merged_df, encoded_csv_, on=['HW_Address', 'ONT_Barcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_pickle(os.path.join(basepath, \"address_encoded.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = combined_df.loc[combined_df['ONT_Barcode'] == 'barcode58']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t['HW_Address'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_addresses = [\n",
    "    'barcode_external08_internal04',\n",
    "    'barcode_external06_internal07',\n",
    "    'barcode_external06_internal06',\n",
    "    'barcode_external05_internal06',\n",
    "    'barcode_external07_internal07'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t.loc[t['HW_Address'].isin(hw_addresses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.to_csv(os.path.join(basepath, \"5add_encoded.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01-04 splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\sequencing_runs\\01-04run\\finetuning_datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\sequencing_runs\\01-04run\\finetuning_datasets\\edit_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_df = df.loc[df['strand'].str.startswith('+')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_df = df.loc[df['strand'].str.startswith('-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_df.to_pickle(os.path.join(basepath, 'edit_forward.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_df.to_pickle(os.path.join(basepath, 'edit_reverse.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower concentration EIC04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = pd.read_csv(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\motif_search_barcoded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms['ONT_Barcode'] = ms['ONT_Barcode'].apply(lambda x: int(x[-2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms['ONT_Barcode'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_barcodes = np.arange(1, 80, 4)\n",
    "t2_barcodes = np.arange(2, 80, 4)\n",
    "t3_barcodes = np.arange(3, 80, 4)\n",
    "t4_barcodes = np.arange(4, 81, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_df = ms.loc[ms['ONT_Barcode'].isin(t2_barcodes)]\n",
    "t3_df = ms.loc[ms['ONT_Barcode'].isin(t3_barcodes)]\n",
    "t4_df = ms.loc[ms['ONT_Barcode'].isin(t4_barcodes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_barcodes = [80, 76, 48, 68, 79, 75, 47, 67, 78, 74, 46, 66]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_encoded = pd.read_csv(r\"C:\\Users\\Parv\\Downloads\\EIC01-01-1280-T2_encoded.tsv\", sep='\\t')\n",
    "t3_encoded = pd.read_csv(r\"C:\\Users\\Parv\\Downloads\\EIC01-01-1280-T3_encoded.tsv\", sep='\\t')\n",
    "t4_encoded = pd.read_csv(r\"C:\\Users\\Parv\\Downloads\\EIC01-01-1280-T4_encoded.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_functions import get_cleaned_encoded_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_encoded = get_cleaned_encoded_file(t2_encoded, address=False)\n",
    "t3_encoded = get_cleaned_encoded_file(t3_encoded, address=False)\n",
    "t4_encoded = get_cleaned_encoded_file(t4_encoded, address=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected barcodes - 80, 76, 48, 68 (4)\n",
    "                    79, 75, 47, 67 (3)\n",
    "                    78, 74, 46, 66 (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get encoded for each and filter out those barcodes, that's your final file - its not a lot of reads to be fair, maybe 10k ish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the same from the fastq, split into files and get edit-search outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use the df to get the squiggles and the same from the cluster (but use a bigger df rather than 4 of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = ms.loc[ms['ONT_Barcode'].isin(selected_barcodes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_1 = pd.merge(ms, t2_encoded, on=['HW_Address', 'ONT_Barcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_2 = pd.merge(ms, t3_encoded, on=['HW_Address', 'ONT_Barcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_3 = pd.merge(ms, t4_encoded, on=['HW_Address', 'ONT_Barcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.concat([merged_1, merged_2, merged_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.to_pickle(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\full_datasets\\diluted_EIC04.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diluted = pd.read_pickle(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\full_datasets\\diluted_EIC04.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
