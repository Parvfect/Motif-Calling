{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from nn import MotifCaller, NaiveCaller\n",
    "from training_data import data_preproc, load_training_data\n",
    "from utils import get_savepaths\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from greedy_decoder import GreedyCTCDecoder\n",
    "from Levenshtein import ratio\n",
    "from utils import load_model, get_metrics_for_evaluation\n",
    "from transcript_sorting import sort_transcript_reduced_spacers, sort_transcript\n",
    "from sklearn.model_selection import train_test_split\n",
    "from beam_search_decoder import beam_search_ctc, torch_ctc\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from sklearn.preprocessing import normalize\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from beam_search_decoder import beam_search_ctc, torch_ctc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading models and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 19\n",
    "model_path_forward = r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\models\\empirical\\final_models\\latest_models\\edit_forward.pth\"\n",
    "model_path_mixed = r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\models\\empirical\\final_models\\latest_models\\edit_mixed.pth\"\n",
    "model_path_reverse = r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\models\\empirical\\final_models\\latest_models\\edit_reverse.pth\"\n",
    "\n",
    "labels_int = np.arange(n_classes).tolist()\n",
    "labels = [f\"{i}\" for i in labels_int] # Tokens to be fed into greedy decoder\n",
    "greedy_decoder = GreedyCTCDecoder(labels = labels)\n",
    "ctc = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_device(device)\n",
    "model_forward = load_model(model_path=model_path_forward, device=device, n_classes=n_classes, hidden_size=256)\n",
    "model_reverse = load_model(model_path=model_path_reverse, device=device, n_classes=n_classes, hidden_size=256)\n",
    "model_mixed = load_model(model_path=model_path_mixed, device=device, n_classes=n_classes, hidden_size=256)\n",
    "\n",
    "test_dataset_path = r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\full_datasets\\diluted_squiggles.pkl\"\n",
    "\n",
    "#test_dataset_path = r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\sequencing_runs\\01-04run\\misc_datasets\\small_squiggle_df.pkl\"\n",
    "#test_dataset_path = r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\sequencing_runs\\01-04run\\finetuning_datasets\\edit_train.pkl\"\n",
    "\n",
    "dataset = pd.read_pickle(test_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the new datasets\n",
    "X, y = load_training_data(\n",
    "        test_dataset_path, column_x='squiggle', column_y='payload', payload=False, orientation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_test, payloads_train, payloads_test = train_test_split(\n",
    "        X, payloads, test_size=0.2, random_state=42)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_by_cycle_and_motif(preds, actual):\n",
    "    cycles = np.zeros(8)\n",
    "    motifs = np.zeros(8)\n",
    "\n",
    "    \n",
    "    for pred, act in zip(preds, actual):\n",
    "        counter = 0\n",
    "        for i, j in zip(pred, act):\n",
    "            for k in i:\n",
    "                if k not in j:\n",
    "                    cycles[counter] += 1\n",
    "                    motifs[k-1] += 1\n",
    "            counter += 1\n",
    "\n",
    "    return cycles / len(preds) * 100, motifs / len(preds) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prediction(prediction, original):\n",
    "\n",
    "    found = 0\n",
    "    err = 0\n",
    "    for i, j in zip(prediction, original):\n",
    "        for k in range(len(i)):\n",
    "            if i[k] in j:\n",
    "                found += 1\n",
    "            else:\n",
    "                err += 1\n",
    "\n",
    "    return found, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_reverse_oriented_read(transcript_reverse_model):\n",
    "\n",
    "    # Remove payloads, make set, and check that each spacer motif is increasing\n",
    "    no_payload_transcript = [\n",
    "        int(i) for i in transcript_reverse_model if int(i) > 8]\n",
    "\n",
    "    first_spacer = no_payload_transcript[0]\n",
    "    flag = False\n",
    "    for j in no_payload_transcript[1:]:\n",
    "        if not j == first_spacer:\n",
    "            if j < first_spacer:\n",
    "                flag = True\n",
    "            else:\n",
    "                flag = False\n",
    "            break\n",
    "        \n",
    "    return flag\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_and_evaluate_predictions(\n",
    "        model_outputs_forward, labels, payload, model_outputs_reverse=None, model_outputs_mixed=None, orientation=None, beam=False, beam_width=10, prob_threshold=0.0):\n",
    "    \n",
    "    original = payload\n",
    "    \n",
    "    if beam:\n",
    "        transcript_forward = torch_ctc(\n",
    "            n_classes=19, model_output=model_outputs_forward.unsqueeze(0),\n",
    "            beam_width=beam_width)\n",
    "    else:\n",
    "        result_forward, quality_forward = greedy_decoder.forward_with_quality(\n",
    "            model_outputs_forward, prob_threshold=prob_threshold)\n",
    "        transcript_forward = \" \".join(result_forward)\n",
    "    sorted_forward = sort_transcript(transcript_forward)\n",
    "    \n",
    "        \n",
    "    if beam:\n",
    "        transcript_reverse = torch_ctc(\n",
    "            n_classes=19, model_output=model_outputs_reverse.unsqueeze(0),\n",
    "            beam_width=beam_width)\n",
    "    else:\n",
    "        result_reverse, quality_reverse = greedy_decoder.forward_with_quality(\n",
    "            model_outputs_reverse, prob_threshold=prob_threshold)\n",
    "        transcript_reverse = \" \".join(result_reverse)\n",
    "    sorted_reverse = sort_transcript(transcript_reverse)\n",
    "\n",
    "    actual_transcript = \" \".join([str(i) for i in labels])\n",
    "    sorted_actual = sort_transcript(actual_transcript)\n",
    "\n",
    "    #motifs_found_search, motif_errs_search = evaluate_prediction(\n",
    "    #    sorted_actual, original)\n",
    "\n",
    "    motifs_found_search, motif_errs_search = [], []\n",
    "\n",
    "    result_mixed = greedy_decoder.forward(model_outputs_mixed)\n",
    "    \n",
    "    motifs_found_caller_forward, motif_errs_caller_forward = evaluate_prediction(\n",
    "        sorted_forward, original)\n",
    "    \n",
    "    if model_outputs_reverse is not None:\n",
    "        motifs_found_caller_reverse, motif_errs_caller_reverse = evaluate_prediction(\n",
    "            sorted_reverse, original)\n",
    "        \"\"\"\n",
    "        if orientation is not None:\n",
    "            if orientation == 1:\n",
    "                return motifs_found_caller_forward, motif_errs_caller_forward, motifs_found_search, motif_errs_search, quality_forward, sorted_forward, 1\n",
    "            else:\n",
    "                return motifs_found_caller_reverse, motif_errs_caller_reverse, motifs_found_search, motif_errs_search, quality_reverse, sorted_reverse, 0\n",
    "        \"\"\"\n",
    "        if detect_reverse_oriented_read(result_mixed):\n",
    "            return motifs_found_caller_reverse, motif_errs_caller_reverse, motifs_found_search, motif_errs_search, quality_reverse, sorted_reverse, 0\n",
    "        else:\n",
    "            return motifs_found_caller_forward, motif_errs_caller_forward, motifs_found_search, motif_errs_search, quality_forward, sorted_forward, 1\n",
    "        \n",
    "    return motifs_found_caller_forward, motif_errs_caller_forward, motifs_found_search, motif_errs_search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batched evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = dataset['squiggle'].to_list()\n",
    "#y_test = dataset['motif_seq'].to_list()\n",
    "#edit_seq = dataset['edit_search_seq'].to_list()  # edit_search_seq\n",
    "#read_ids_ = dataset['read_id'].to_list()\n",
    "payloads_test = dataset['payload'].to_list()  # payload_seq\n",
    "#orientations = dataset['orientation_x'].to_list()  # strand\n",
    "#orientations = [1 if i.startswith('+') else 0 for i in orientations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payloads_test = dataset['payload'].apply(lambda x: list(eval(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_test_samples = len(X_test)\n",
    "batch_size = 8\n",
    "prob_threshold = 0.85\n",
    "\n",
    "\n",
    "results_dict = {\n",
    "    \"mf_caller\": [],\n",
    "    \"me_caller\": [],\n",
    "    \"mf_search\": [],\n",
    "    \"me_search\": [],\n",
    "    \"mf_edit\": [],\n",
    "    \"me_edit\": [],\n",
    "    \"orientation\": [],\n",
    "    \"quality\": []\n",
    "}\n",
    "orientations_as_you_go = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for ind in tqdm(range(0, n_test_samples, batch_size)):\n",
    "\n",
    "        if n_test_samples - ind < batch_size:\n",
    "            continue\n",
    "        \n",
    "        input_seqs = [\n",
    "            normalize([X_test[k]], norm='max').flatten() for k in range(ind, ind + batch_size)]\n",
    "        \n",
    "        input_seqs = pad_sequence([torch.tensor(\n",
    "                    i, dtype=torch.float32) for i in input_seqs], batch_first=True)\n",
    "        \n",
    "        input_seqs = input_seqs.view(input_seqs.shape[0], 1, input_seqs.shape[1])\n",
    "        \n",
    "        \n",
    "        model_output_forward = model_forward(input_seqs).detach().cpu()\n",
    "        model_output_reverse = model_reverse(input_seqs).detach().cpu()\n",
    "        model_output_mixed = model_mixed(input_seqs).detach().cpu()\n",
    "\n",
    "        for k in range(batch_size):\n",
    "            #orientation = orientations[ind + k]\n",
    "\n",
    "            \n",
    "            motifs_found_caller, motif_errs_caller, motifs_found_search, motif_errs_search, quality, transcript, orientation = sort_and_evaluate_predictions(\n",
    "            model_outputs_forward=model_output_forward[k], labels=[],\n",
    "            payload=payloads_test[ind + k], model_outputs_reverse=model_output_reverse[k],\n",
    "            model_outputs_mixed=model_output_mixed[k], orientation=False, beam=False,\n",
    "            beam_width=15, prob_threshold=prob_threshold)\n",
    "\n",
    "            #motifs_found_edit, motif_errs_edit = evaluate_prediction(\n",
    "            #    edit_seq[ind + k][2:], payloads_test[ind + k])\n",
    "            \n",
    "            results_dict['mf_caller'].append(motifs_found_caller)\n",
    "            results_dict['me_caller'].append(motif_errs_caller)\n",
    "            #results_dict['mf_search'].append(motifs_found_search)\n",
    "            #results_dict['me_search'].append(motif_errs_search)\n",
    "            #results_dict['mf_edit'].append(motifs_found_edit)\n",
    "            #results_dict['me_edit'].append(motif_errs_edit)\n",
    "            results_dict['orientation'].append(orientation)\n",
    "            results_dict['quality'].append(quality)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['mf_caller', 'me_caller', 'orientation', 'quality']\n",
    "results_df = pd.DataFrame({i: results_dict[i] for i in columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_threshold = 11\n",
    "\n",
    "filtered_df = results_df.loc[results_df['quality'] > quality_threshold]\n",
    "\n",
    "mean_values = filtered_df.mean(numeric_only=True)\n",
    "\n",
    "print(\"Mean metrics combined:\")\n",
    "print(mean_values.to_frame(name='Mean').T)\n",
    "print()\n",
    "\n",
    "mean_values = filtered_df.loc[filtered_df['orientation'] == 1].mean(numeric_only=True)\n",
    "print(\"Mean metrics forward\")\n",
    "print(mean_values.to_frame(name='Mean').T)\n",
    "print()\n",
    "\n",
    "mean_values = filtered_df.loc[filtered_df['orientation'] == 0].mean(numeric_only=True)\n",
    "print(\"Mean metrics reverse\")\n",
    "print(mean_values.to_frame(name='Mean').T)\n",
    "print()\n",
    "\n",
    "print(f\"{len(filtered_df) / len(results_df) * 100}% of read pool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef = pd.read_pickle(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\sequencing_runs\\01-04run\\finetuning_datasets\\edit_forward_01_04.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6.68/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_reads = results_df.loc[results_df['orientation'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting inference outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_test_samples = len(X_test)\n",
    "batch_size = 8\n",
    "prob_threshold = 0\n",
    "sorted_caller = []\n",
    "qualities = []\n",
    "full_qs = []\n",
    "caller_orientations = []\n",
    "greedy_transcripts = []\n",
    "read_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for ind in tqdm(range(0, n_test_samples, batch_size)):\n",
    "\n",
    "        if n_test_samples - ind < batch_size:\n",
    "            continue\n",
    "        \n",
    "        input_seqs = [\n",
    "            normalize([X_test[k]], norm='max').flatten() for k in range(ind, ind + batch_size)]\n",
    "        \n",
    "        input_seqs = pad_sequence([torch.tensor(\n",
    "                    i, dtype=torch.float32) for i in input_seqs], batch_first=True)\n",
    "        \n",
    "        input_seqs = input_seqs.view(input_seqs.shape[0], 1, input_seqs.shape[1])\n",
    "        \n",
    "        model_output_mixed = model_mixed(input_seqs).detach().cpu()\n",
    "\n",
    "        \n",
    "        model_output_forward = model_forward(input_seqs).detach().cpu()\n",
    "        model_output_mixed = model_mixed(input_seqs).detach().cpu()\n",
    "        model_output_reverse = model_reverse(input_seqs).detach().cpu()\n",
    "        \n",
    "\n",
    "        for k in range(batch_size):\n",
    "\n",
    "            greedy_result_mixed = greedy_decoder(\n",
    "                model_output_mixed[k]\n",
    "            )\n",
    "\n",
    "            if detect_reverse_oriented_read(greedy_result_mixed):\n",
    "                caller_orientations.append(0)\n",
    "                greedy_result_reverse, quality, full_q = greedy_decoder.forward_with_quality(model_output_reverse[k], prob_threshold=prob_threshold, full_qualities=True)\n",
    "                greedy_transcript = \" \".join(greedy_result_reverse)\n",
    "            else:\n",
    "                caller_orientations.append(1)\n",
    "                greedy_result_forward, quality, full_q = greedy_decoder.forward_with_quality(\n",
    "                model_output_forward[k], prob_threshold=prob_threshold, full_qualities=True)\n",
    "                greedy_transcript = \" \".join(greedy_result_forward)\n",
    "\n",
    "            sorted_greedy = sort_transcript(greedy_transcript)\n",
    "\n",
    "            sorted_caller.append(sorted_greedy)\n",
    "            qualities.append(quality)\n",
    "            full_qs.append(full_q)\n",
    "            greedy_transcripts.append(greedy_transcript)\n",
    "            read_ids.append(ind + k)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs = [\n",
    "            normalize([X_test[k]], norm='max').flatten() for k in range(ind, ind + batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(dataset.tail(4).index,\n",
    "        inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sorted_caller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_caller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['caller_orientations'] = caller_orientations\n",
    "dataset['caller_seq'] = sorted_caller\n",
    "dataset['qualities'] = qualities\n",
    "dataset['full_q'] = full_qs\n",
    "dataset['greedy_transcripts'] = greedy_transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, row in dataset.iterrows():\n",
    "\n",
    "    print(evaluate_prediction(row['caller_seq'], row['payload_seq']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['full_q'] = dataset['full_q'].apply(lambda x: [float(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(columns=['squiggle'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pickle(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\full_datasets\\master_test_labelled.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacers classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacers\n",
    "\n",
    "filtered_df = dataset.loc[dataset['orientation'].str.startswith('+')]\n",
    "\n",
    "y_test = dataset['edit_spacer_seq']\n",
    "\n",
    "n_training_samples = len(X_test)\n",
    "batch_size = 8\n",
    "diff = 0\n",
    "n_test_samples = 100\n",
    "motifs_found_caller_arr = []\n",
    "motif_errs_caller_arr = []\n",
    "motifs_found_search_arr = []\n",
    "motif_errs_search_arr = []\n",
    "model = model_forward.to(device)\n",
    "\n",
    "matched_spacers_arr = []\n",
    "unique_spacers_arr = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for ind in tqdm(range(0, n_training_samples, batch_size)):\n",
    "\n",
    "        if n_training_samples - ind < batch_size:\n",
    "            continue\n",
    "        \n",
    "        input_seqs = [\n",
    "            normalize([X_test[k]], norm='max').flatten() for k in range(ind, ind + batch_size)]\n",
    "        \n",
    "        target_seqs = y_test[ind: ind + batch_size]\n",
    "\n",
    "        input_seqs = pad_sequence([torch.tensor(\n",
    "                    i, dtype=torch.float32) for i in input_seqs], batch_first=True)\n",
    "        target_seqs = pad_sequence([torch.tensor(\n",
    "                    i, dtype=torch.float32) for i in target_seqs], batch_first=True)\n",
    "        \n",
    "        input_seqs = input_seqs.view(input_seqs.shape[0], 1, input_seqs.shape[1])\n",
    "        #input_seqs = input_seqs.to(device)\n",
    "        \n",
    "        pad_length_input = input_seqs.shape[2]\n",
    "        n_samples = input_seqs.shape[0]\n",
    "\n",
    "        pad_length_target = target_seqs.shape[1]\n",
    "\n",
    "        model_output = model(input_seqs)\n",
    "        model_output = model_output.permute(1, 0, 2)  # Assuming log probs are computed in network\n",
    "        \n",
    "        \n",
    "        n_timesteps = model_output.shape[0]\n",
    "        input_lengths = torch.tensor([n_timesteps for i in range(n_samples)])\n",
    "        label_lengths = torch.tensor([len(y_test[ind + i]) for i in range(n_samples)])\n",
    "        \n",
    "        \n",
    "        \n",
    "        model_output = model_output.permute(1, 0, 2).detach().cpu()\n",
    "        #print(model_output.shape)\n",
    "\n",
    "        for k in range(batch_size):\n",
    "            original = payloads_test[ind + k]\n",
    "            greedy_result = greedy_decoder(model_output[k])\n",
    "            spacers = [int(i) for i in greedy_result if int(i) > 8]\n",
    "            actual_spacers =([i for i in y_test[ind + k] if i > 8])\n",
    "            matched_spacers = set(spacers).intersection(actual_spacers)\n",
    "            unique_spacers = set(spacers)  - set(actual_spacers)\n",
    "            matched_spacers_arr.append(len(matched_spacers))\n",
    "            unique_spacers_arr.append(len(unique_spacers))\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "        \"\"\"\n",
    "        if ind >= n_test_samples:\n",
    "            print(diff / (n_test_samples))\n",
    "            break\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(matched_spacers_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(unique_spacers_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6.82/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(matched_spacers_arr) / (np.mean(unique_spacers_arr) + np.mean(matched_spacers_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box_stats(data):\n",
    "    data = np.array(data)\n",
    "    median = np.median(data)\n",
    "    upper_quartile = np.percentile(data, 75)\n",
    "    lower_quartile = np.percentile(data, 25)\n",
    "\n",
    "    iqr = upper_quartile - lower_quartile\n",
    "    upper_whisker = data[data<=upper_quartile+1.5*iqr].max()\n",
    "    lower_whisker = data[data>=lower_quartile-1.5*iqr].min()\n",
    "    return median, upper_quartile, lower_quartile, upper_whisker, lower_whisker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_caller = results_df['mf_edit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_caller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_box_stats(mf_caller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['mf_caller', 'mf_edit', 'mf_search']\n",
    "box_stat_arr = []\n",
    "\n",
    "for i in columns:\n",
    "    if i == 'mf_caller':\n",
    "        filtered_df = results_df.loc[results_df['quality'] > 12]\n",
    "    else:\n",
    "        filtered_df = results_df\n",
    "    detected_arr = filtered_df[i]\n",
    "    detected_arr *= 100/8\n",
    "    box_stat = get_box_stats(detected_arr)\n",
    "    box_stat_arr.append(box_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_stat_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = results_df.loc[results_df['quality'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motifs_found_search_arr_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 1/ (8) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motifs_found_caller_arr_ = [i * frac for i in motifs_found_caller_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(motifs_found_caller_arr_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motifs_found_search_arr_ = [i * frac for i in motifs_found_search_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.DataFrame([motifs_found_caller_arr_, motifs_found_search_arr_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.to_csv('motifs_found_empirical.csv', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tallies(tallies, prediction):\n",
    "\n",
    "    for ind, i in enumerate(prediction):\n",
    "        for j in i:\n",
    "            tallies[ind][j-1] += 1\n",
    "\n",
    "    return tallies\n",
    "\n",
    "def evaluate_motif_tallies(motif_tallies: List[List[int]], payload_cycles: List[List[int]]):\n",
    "    correct = 0\n",
    "    errs = 0\n",
    "    for tallies, cycle in zip(motif_tallies, payload_cycles):\n",
    "        sorted_tallies = np.argsort(tallies)[::-1]\n",
    "        top_4 = [i+1 for i in sorted_tallies[:4]]\n",
    "        correct += len(set(top_4).intersection(set(cycle)))\n",
    "        errs += len(set(top_4) - set(cycle))\n",
    "\n",
    "    return correct / 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_prediction_batched(\n",
    "        squiggles: List[List[float]], batch_size:int, beam: bool, beam_width: int = 30, prob_threshold:float = 0.0) -> List[List[int]]:\n",
    "\n",
    "    greedy_transcripts = []\n",
    "    qualities = []\n",
    "    input_seqs = [\n",
    "        normalize([squiggles[k]], norm='max').flatten() for k in range(\n",
    "            len(squiggles))]\n",
    "    \n",
    "    input_seqs = pad_sequence([torch.tensor(\n",
    "                i, dtype=torch.float32) for i in input_seqs], batch_first=True)\n",
    "    \n",
    "    input_seqs = input_seqs.view(\n",
    "        input_seqs.shape[0], 1, input_seqs.shape[1])\n",
    "\n",
    "    model_output_mixed = model_mixed(input_seqs).detach().cpu()\n",
    "\n",
    "    for k in range(batch_size):\n",
    "        greedy_result_mixed = greedy_decoder.forward(model_output_mixed[k])\n",
    "\n",
    "        if detect_reverse_oriented_read(greedy_result_mixed):\n",
    "            model_output_reverse = model_reverse(input_seqs).detach().cpu()\n",
    "            greedy_result, quality = greedy_decoder.forward_with_quality(\n",
    "                model_output_reverse[k], prob_threshold=prob_threshold)\n",
    "        else:\n",
    "            model_output_forward = model_forward(input_seqs).detach().cpu()\n",
    "            greedy_result, quality = greedy_decoder.forward_with_quality(\n",
    "                model_output_forward[k], prob_threshold=prob_threshold\n",
    "            )\n",
    "    \n",
    "        greedy_transcript = \" \".join(greedy_result)\n",
    "        sorted_greedy = sort_transcript(greedy_transcript)\n",
    "        \n",
    "        greedy_transcripts.append(sorted_greedy)\n",
    "        qualities.append(quality)\n",
    "\n",
    "    return greedy_transcripts, qualities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barcoded runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_barcodes = np.arange(2, 81, 4)\n",
    "t3_barcodes = np.arange(3, 81, 4)\n",
    "t4_barcodes = np.arange(4, 81, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.loc[test_df['ONT_Barcode'].isin(t4_barcodes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = test_df['squiggle'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_ids_ = test_df['read_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caller_predictions_arr = []\n",
    "search_predictions_arr = []\n",
    "edit_predictions_arr = []\n",
    "payloads_arr = []\n",
    "batch_size = 8\n",
    "quality_threshold = 0\n",
    "prob_threshold = 0.85\n",
    "\n",
    "# Testing convergence - add quality filtering..\n",
    "for barcode in tqdm(test_df['ONT_Barcode'].unique()):\n",
    "    u_cycle = 0\n",
    "    for cycle in tqdm(test_df['HW_Address'].unique()):\n",
    "        selected_df = test_df.loc[\n",
    "            (test_df['ONT_Barcode'] == barcode) &\n",
    "            (test_df['HW_Address'] == cycle)\n",
    "        ]\n",
    "        read_ids = selected_df['read_id'].tolist()\n",
    "        if len(read_ids) == 0:\n",
    "            continue\n",
    "        payload = selected_df['payload'].tolist()[0]\n",
    "        squiggles = [X[read_ids_.index(k)] for k in read_ids]\n",
    "        #search_predictions = selected_df['motif_seq'].tolist()\n",
    "        #search_predictions_sorted = [\n",
    "        #    sort_transcript(i) for i in selected_df['motif_seq'].tolist()]\n",
    "        decoded_predictions = []\n",
    "\n",
    "        prediction_indices = []\n",
    "        \n",
    "        for ind in range(0, len(squiggles), batch_size):\n",
    "            if len(squiggles) - ind < batch_size:\n",
    "                model_prediction, qualities = get_model_prediction_batched(\n",
    "                squiggles[ind:],\n",
    "                batch_size=len(squiggles) - ind,\n",
    "                beam=False, prob_threshold=prob_threshold)\n",
    "            else:\n",
    "                model_prediction, qualities = get_model_prediction_batched(\n",
    "                    squiggles[ind: ind + batch_size],\n",
    "                    batch_size=batch_size,\n",
    "                    beam=False, prob_threshold=prob_threshold)\n",
    "                \n",
    "            selected_indices = [\n",
    "                i for i in range(len(qualities)) if qualities[i] > quality_threshold]\n",
    "            \n",
    "            prediction_indices.extend([i + ind * batch_size for i in selected_indices])\n",
    "            \n",
    "            #for i in model_prediction:\n",
    "            #    print(evaluate_prediction(i, payload))\n",
    "            decoded_predictions.extend([model_prediction[i] for i in selected_indices])\n",
    "            \n",
    "            if len(prediction_indices) > 80:\n",
    "                break\n",
    "\n",
    "        \n",
    "        #search_predictions_arr.append(search_predictions_sorted[:len(decoded_predictions)])\n",
    "        caller_predictions_arr.append(decoded_predictions)\n",
    "        #edit_predictions_arr.append([i[2:] for i in selected_df['edit_search_seq'][:len(decoded_predictions)]])\n",
    "        \n",
    "        payloads_arr.append(payload)\n",
    "        u_cycle += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-barcoded runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['squiggle'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For the non barcoded runs\n",
    "\n",
    "caller_predictions_arr = []\n",
    "#search_predictions_arr = []\n",
    "payloads_arr = []\n",
    "batch_size = 8\n",
    "quality_threshold = 12\n",
    "prob_threshold = 0.85\n",
    "\n",
    "\n",
    "for cycle in tqdm(test_df['HW_Address'].unique()):\n",
    "    selected_df = test_df.loc[\n",
    "        (test_df['HW_Address'] == cycle) #&\n",
    "        #(test_df['strand'].str.startswith('+')) \n",
    "    ]\n",
    "    read_ids = selected_df['read_id'].tolist()\n",
    "    payload = selected_df['payload'].tolist()[0]\n",
    "    squiggles = [X[read_ids_.index(k)] for k in read_ids]\n",
    "    #search_predictions = selected_df['motif_seq'].tolist()\n",
    "    #search_predictions_sorted = [\n",
    "    #    sort_transcript(i) for i in selected_df['motif_seq'].tolist()]\n",
    "    decoded_predictions = []\n",
    "    \n",
    "    for ind in tqdm(range(0, len(squiggles), batch_size)):\n",
    "        if len(squiggles) - ind < batch_size:\n",
    "            model_prediction, qualities = get_model_prediction_batched(\n",
    "            squiggles[ind:],\n",
    "            search_predictions[ind:],\n",
    "            batch_size=len(squiggles) - ind,\n",
    "            beam=False, prob_threshold=prob_threshold)\n",
    "        else:\n",
    "            model_prediction, qualities = get_model_prediction_batched(\n",
    "                squiggles[ind: ind + batch_size],\n",
    "                batch_size=batch_size,\n",
    "                beam=False, prob_threshold=prob_threshold)\n",
    "        \n",
    "        selected_indices = [\n",
    "                i for i in range(len(qualities)) if qualities[i] > quality_threshold]\n",
    "            \n",
    "        #prediction_indices.extend([i + ind * batch_size for i in selected_indices])\n",
    "            \n",
    "        #for i in model_prediction:\n",
    "        #    print(evaluate_prediction(i, payload))\n",
    "        decoded_predictions.extend([model_prediction[i] for i in selected_indices])\n",
    "\n",
    "        if len(decoded_predictions) > 80:\n",
    "            break\n",
    "    \n",
    "    #search_predictions_arr.append(search_predictions_sorted[:len(decoded_predictions)])\n",
    "    caller_predictions_arr.append(decoded_predictions)\n",
    "    \n",
    "    payloads_arr.append(payload)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running consensus decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_4_motifs(tallies: dict):\n",
    "    prediction = []\n",
    "    for i in range(len(tallies)):\n",
    "        elems = heapq.nlargest(4, tallies[i].values())\n",
    "        inds = heapq.nlargest(4, tallies[i].keys(), key = lambda x: tallies[i][x])\n",
    "        prediction.append([k for j, k in zip(elems, inds) if j > 0 ])\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_accs_arr = []\n",
    "caller_accs_arr = []\n",
    "edit_accs_arr = []\n",
    "\n",
    "\n",
    "### Comment out when non-barcoded\n",
    "search_predictions_arr = caller_predictions_arr\n",
    "edit_predictions_arr = caller_predictions_arr\n",
    "\n",
    "for search_prediction, caller_prediction, edit_prediction, payload in zip(\n",
    "    search_predictions_arr, caller_predictions_arr, edit_predictions_arr, payloads_arr):\n",
    "\n",
    "    n_reads = 0\n",
    "    motif_tallies_caller = [{i: 0 for i in range(1, 9)} for i in range(8)]\n",
    "    motif_tallies_search = [{i: 0 for i in range(1, 9)} for i in range(8)]\n",
    "    motif_tallies_edit = [{i: 0 for i in range(1, 9)} for i in range(8)]\n",
    "    search_accs, caller_accs, edit_accs = [], [], []\n",
    "    for i, j, k in zip(search_prediction, caller_prediction, edit_prediction):  # for each prediction\n",
    "        cycle_num = 0\n",
    "        for search_cycle, caller_cycle, edit_cycle in zip(i, j, k):  # loop over cycles and update tallies\n",
    "            for motif in search_cycle:\n",
    "                motif_tallies_search[cycle_num][motif] += 1\n",
    "            for motif in caller_cycle:\n",
    "                motif_tallies_caller[cycle_num][motif] += 1\n",
    "            for motif in edit_cycle:\n",
    "                motif_tallies_edit[cycle_num][motif] += 1\n",
    "            cycle_num += 1\n",
    "\n",
    "        #  Get caller and search predictions\n",
    "        caller_prediction_ = get_top_4_motifs(motif_tallies_caller)\n",
    "        search_prediction_ = get_top_4_motifs(motif_tallies_search)\n",
    "        edit_prediction_ = get_top_4_motifs(motif_tallies_edit)\n",
    "        \n",
    "        n_reads += 1\n",
    "\n",
    "        #print(payload)\n",
    "        #payload = eval(payload)  # comment out for other datasets\n",
    "        #  Evaluate predictions\n",
    "        search_acc = evaluate_prediction(search_prediction_, payload)[0]/32\n",
    "        caller_acc  = evaluate_prediction(caller_prediction_, payload)[0]/32\n",
    "        edit_acc = evaluate_prediction(edit_prediction_, payload)[0]/32\n",
    "        \n",
    "\n",
    "        search_accs.append(search_acc)\n",
    "        caller_accs.append(caller_acc)\n",
    "        edit_accs.append(edit_acc)\n",
    "        \n",
    "    \n",
    "    search_accs_arr.append(search_accs)\n",
    "    caller_accs_arr.append(caller_accs)\n",
    "    edit_accs_arr.append(edit_accs)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([i[-1] for i in caller_accs_arr if len(i) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_search = []\n",
    "avg_caller = []\n",
    "avg_edit = []\n",
    "\n",
    "for i in range(7):\n",
    "\n",
    "    search_sum = 0\n",
    "    search_samples = 0\n",
    "    for k in search_accs_arr:\n",
    "        if i < len(k):\n",
    "            search_sum += k[i]\n",
    "            search_samples += 1\n",
    "    \n",
    "    caller_sum = 0\n",
    "    caller_samples = 0\n",
    "    for k in caller_accs_arr:\n",
    "        if i < len(k):\n",
    "            caller_sum += k[i]\n",
    "            caller_samples += 1\n",
    "\n",
    "    edit_sum = 0\n",
    "    edit_samples = 0\n",
    "    for k in edit_accs_arr:\n",
    "        if i < len(k):\n",
    "            edit_sum += k[i]\n",
    "            edit_samples += 1\n",
    "\n",
    "    avg_search.append(search_sum / search_samples)\n",
    "    avg_caller.append(caller_sum / caller_samples)\n",
    "    avg_edit.append(edit_sum / edit_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(avg_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['ONT_Barcode'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "\n",
    "for i in test_df['ONT_Barcode'].unique():\n",
    "    sum += test_df.loc[test_df['ONT_Barcode'] == i]['HW_Address'].value_counts().sum()/ 64\n",
    "\n",
    "sum/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_caller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_search, label='zero-error search')\n",
    "plt.plot(avg_caller, label='motif caller')\n",
    "plt.plot(avg_edit, label='edit search')\n",
    "plt.grid()\n",
    "plt.yticks(np.arange(0, 1.01, 0.05))\n",
    "plt.xticks(np.arange(0, 50, 2))\n",
    "plt.xlim(0, 50)\n",
    "plt.xlabel(\"Number of reads\")\n",
    "plt.ylabel(\"Recovery percentage\")\n",
    "plt.title(\"Convergence of motif-inferring methods\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit-distance search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\full_datasets\\master_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\edit_distance_motif_search\\res-loose-chain.txt\", 'r') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_ids = []\n",
    "positions = []\n",
    "orientations = []\n",
    "motifs_found = []\n",
    "\n",
    "for i in lines:\n",
    "    split_i = i.split()\n",
    "    read_ids.append(split_i[0][3:])\n",
    "    orientations.append(split_i[1])\n",
    "    positions.append(split_i[2][4:].split('-'))\n",
    "    motifs_found.append(split_i[4][8:].split('->'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motifs_found_ = [[int(j[1]) for j in i if not j == 'fake' and j.startswith('m')] for i in motifs_found ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_distance_df = pd.DataFrame({'read_id': read_ids, 'positions': positions, 'orientations': orientations, 'edit_motif_seq': motifs_found_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_distance_df.to_pickle(r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\data\\empirical\\edit_distance_motif_search.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df, edit_distance_df, on='read_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transcript_sorting import sort_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in merged_df.iterrows():\n",
    "    payload_seq = row['payload_seq']\n",
    "    search_seq = row['motif_seq']\n",
    "    edit_seq = row['edit_motif_seq']\n",
    "    orientation = row['orientation']\n",
    "    \n",
    "    if len(edit_seq) <= 8:\n",
    "        print(evaluate_prediction([[i] for i in edit_seq], payload_seq))\n",
    "        print(evaluate_prediction(sort_transcript(search_seq), payload_seq))\n",
    "        print()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
